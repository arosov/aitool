#!/usr/bin/env python3

import subprocess
import requests
import json
import re
import sys
import time
import io

from rich import box
from rich.console import Console
from rich.live import Live
from rich.markdown import CodeBlock, Heading, Markdown
from rich.panel import Panel
from rich.syntax import Syntax
from rich.text import Text

# Set to True to enable verbose debugging output to stderr.
DEBUG_MODE = False

# Configuration for the Llamafile server
LLAMA_SERVICE_NAME = "llamaserver.service"
LLAMA_SERVER_URL = "http://127.0.0.1:4141/v1/chat/completions"
MODEL_NAME = "gemma3-1b-it-q6k"

class NoInsetCodeBlock(CodeBlock):
    """A code block with syntax highlighting and no padding."""

    def __rich_console__(self, console, options):
        code = str(self.text).rstrip()
        syntax = Syntax(code, self.lexer_name, theme=self.theme, word_wrap=True, padding=(1, 0))
        yield syntax


class LeftHeading(Heading):
    """A heading class that renders left-justified."""

    def __rich_console__(self, console, options):
        text = self.text
        text.justify = "left"  # Override justification
        if self.tag == "h1":
            # Draw a border around h1s, but keep text left-aligned
            yield Panel(
                text,
                box=box.HEAVY,
                style="markdown.h1.border",
            )
        else:
            # Styled text for h2 and beyond
            if self.tag == "h2":
                yield Text("")  # Keep the blank line before h2
            yield text


class NoInsetMarkdown(Markdown):
    """Markdown with code blocks that have no padding and left-justified headings."""

    elements = {
        **Markdown.elements,
        "fence": NoInsetCodeBlock,
        "code_block": NoInsetCodeBlock,
        "heading_open": LeftHeading,
    }



class MarkdownStream:
    """Streaming markdown renderer that progressively displays content with a live updating window.

    Uses rich.console and rich.live to render markdown content with smooth scrolling
    and partial updates. Maintains a sliding window of visible content while streaming
    in new markdown text.
    """

    live = None  # Rich Live display instance
    when = 0  # Timestamp of last update
    min_delay = 1.0 / 20  # Minimum time between updates (20fps)
    live_window = 6  # Number of lines to keep visible at bottom during streaming

    def __init__(self, mdargs=None):
        """Initialize the markdown stream.

        Args:
            mdargs (dict, optional): Additional arguments to pass to rich Markdown renderer
        """
        self.printed = []  # Stores lines that have already been printed

        if mdargs:
            self.mdargs = mdargs
        else:
            self.mdargs = dict()

        # Defer Live creation until the first update.
        self.live = None
        self._live_started = False

    def _render_markdown_to_lines(self, text):
        """Render markdown text to a list of lines.

        Args:
            text (str): Markdown text to render

        Returns:
            list: List of rendered lines with line endings preserved
        """
        # Render the markdown to a string buffer
        string_io = io.StringIO()
        console = Console(file=string_io, force_terminal=True)
        markdown = NoInsetMarkdown(text, **self.mdargs)
        console.print(markdown)
        output = string_io.getvalue()

        # Split rendered output into lines
        return output.splitlines(keepends=True)

    def __del__(self):
        """Destructor to ensure Live display is properly cleaned up."""
        if self.live:
            try:
                self.live.stop()
            except Exception:
                pass  # Ignore any errors during cleanup

    def update(self, text, final=False):
        """Update the displayed markdown content.

        Args:
            text (str): The markdown text received so far
            final (bool): If True, this is the final update and we should clean up

        Splits the output into "stable" older lines and the "last few" lines
        which aren't considered stable. They may shift around as new chunks
        are appended to the markdown text.

        The stable lines emit to the console above the Live window.
        The unstable lines emit into the Live window so they can be repainted.

        Markdown going to the console works better in terminal scrollback buffers.
        The live window doesn't play nice with terminal scrollback.
        """
        # On the first call, stop the spinner and start the Live renderer
        if not getattr(self, "_live_started", False):
            self.live = Live(Text(""), refresh_per_second=1.0 / self.min_delay)
            self.live.start()
            self._live_started = True

        now = time.time()
        # Throttle updates to maintain smooth rendering
        if not final and now - self.when < self.min_delay:
            return
        self.when = now

        # Measure render time and adjust min_delay to maintain smooth rendering
        start = time.time()
        lines = self._render_markdown_to_lines(text)
        render_time = time.time() - start

        # Set min_delay to render time plus a small buffer
        self.min_delay = min(max(render_time * 10, 1.0 / 20), 2)

        num_lines = len(lines)

        # How many lines have "left" the live window and are now considered stable?
        # Or if final, consider all lines to be stable.
        if not final:
            num_lines -= self.live_window

        # If we have stable content to display...
        if final or num_lines > 0:
            # How many stable lines do we need to newly show above the live window?
            num_printed = len(self.printed)
            show = num_lines - num_printed

            # Skip if no new lines to show above live window
            if show <= 0:
                return

            # Get the new lines and display them
            show = lines[num_printed:num_lines]
            show = "".join(show)
            show = Text.from_ansi(show)
            self.live.console.print(show)  # to the console above the live area

            # Update our record of printed lines
            self.printed = lines[:num_lines]

        # Handle final update cleanup
        if final:
            self.live.update(Text(""))
            self.live.stop()
            self.live = None
            return

        # Update the live window with remaining lines
        rest = lines[num_lines:]
        rest = "".join(rest)
        rest = Text.from_ansi(rest)
        self.live.update(rest)

    def find_minimal_suffix(self, text, match_lines=50):
        """
        Splits text into chunks on blank lines "\n\n".
        """

def get_manpage_content(command_names_str):
    """Fetches man pages for a comma-separated list of commands."""
    if not command_names_str:
        return ""
    command_names = [cmd.strip() for cmd in command_names_str.split(',') if cmd.strip()]
    all_man_content = []
    for command_name in command_names:
        print(f"Fetching manpage for {command_name}...")
        try:
            process = subprocess.run(
                ["man", command_name],
                capture_output=True,
                text=True,
                check=False,  # Do not raise exception for non-zero exit code
                env={"LANG": "C", "LC_ALL": "C"}  # Ensure consistent 'man' output language
            )
            if process.returncode == 0:
                all_man_content.append(f"--- Manpage for {command_name} ---\n{process.stdout.strip()}")
            else:
                if DEBUG_MODE:
                    print(f"Debug: Could not retrieve manpage for {command_name}. Exit code: {process.returncode}", file=sys.stderr)
                all_man_content.append(f"--- No manpage found or error for {command_name} ---")
        except FileNotFoundError:
            if DEBUG_MODE:
                print(f"Debug: 'man' command not found. Cannot retrieve manpage for {command_name}.", file=sys.stderr)
            all_man_content.append(f"--- 'man' command not found, cannot retrieve manpage for {command_name} ---")
            # If 'man' itself is not found, we might want to stop trying for others,
            # but current loop structure will continue, which is acceptable.
        except Exception as e:
            if DEBUG_MODE:
                print(f"Debug: Error retrieving manpage for {command_name}: {e}", file=sys.stderr)
            all_man_content.append(f"--- Error retrieving manpage for {command_name}: {e} ---")
    
    if not all_man_content:
        return ""
    return "\n\n".join(all_man_content) + "\n"


def check_and_start_service():
    """Checks if the systemd user service is running and starts it if not."""
    try:
        # Check if the service is active
        subprocess.run(
            ["systemctl", "--user", "is-active", "--quiet", LLAMA_SERVICE_NAME],
            check=True,
            capture_output=True
        )
    except subprocess.CalledProcessError:
        # Service is not active, try to start it
        print(f"{LLAMA_SERVICE_NAME} is not running. Attempting to start it...")
        try:
            subprocess.run(
                ["systemctl", "--user", "start", LLAMA_SERVICE_NAME],
                check=True,
                capture_output=True
            )
            # Give it a moment to start
            time.sleep(5)
            subprocess.run(
                ["systemctl", "--user", "is-active", "--quiet", LLAMA_SERVICE_NAME],
                check=True,
                capture_output=True
            )
            print(f"{LLAMA_SERVICE_NAME} started successfully.")
        except subprocess.CalledProcessError as e:
            print(f"Error: Failed to start {LLAMA_SERVICE_NAME}.", file=sys.stderr)
            print(f"Please check 'journalctl --user -u {LLAMA_SERVICE_NAME}' for details.", file=sys.stderr)
            sys.exit(1)

def send_query_to_llm(query: str):
    """Sends a query to the LLM and prints the streaming response."""

    # Construct the JSON payload for the API request
    # The prompt is designed to encourage concise, command-line friendly responses.
    # Set "stream": true to enable streaming responses from the server.
    json_payload = {
        "model": MODEL_NAME,
        "messages": [
            {"role": "system", "content": "You are a helpful assistant that provides concise, direct, and actionable answers, especially regarding shell commands and technical queries. Keep responses brief and to the point. Use 10 sentences to reply at most."},
            {"role": "user", "content": query}
        ],
        "max_tokens": 1500,
        "temperature": 0.2,
        "stream": True
    }

    markdown_output = MarkdownStream()
    output_acc = ""

    try:
        # Send the request to the Llamafile server with streaming enabled
        with requests.post(LLAMA_SERVER_URL, json=json_payload, stream=True) as response:
            response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)

            for line in response.iter_lines():
                if line:
                    decoded_line = line.decode('utf-8')
                    if decoded_line.startswith("data: "):
                        json_chunk_str = decoded_line[len("data: "):]

                        if DEBUG_MODE:
                            print(f"\n--- DEBUG: Raw JSON Chunk ---", file=sys.stderr)
                            print(f"{json_chunk_str}", file=sys.stderr)

                        if json_chunk_str == "[DONE]":
                            break # End of stream

                        try:
                            json_chunk = json.loads(json_chunk_str)
                            # Extract content using .get() with a default empty string
                            # This is equivalent to jq's `// ""`
                            raw_content = json_chunk.get('choices', [{}])[0].get('delta', {}).get('content', '')

                            if DEBUG_MODE:
                                print(f"--- DEBUG: Raw Content from json.loads ---", file=sys.stderr)
                                print(f"'{raw_content}'", file=sys.stderr)

                            # Clean the content: remove <endofturn> and markdown characters
                            # Using re.sub for regex replacement
                            cleaned_content = re.sub(r'<end_of_turn>', '', raw_content, flags=re.IGNORECASE)
                            # Remove leading/trailing whitespace from the chunk, but preserve internal newlines
                            cleaned_content = cleaned_content

                            if DEBUG_MODE:
                                print(f"--- DEBUG: Cleaned Content (preserving newlines) ---", file=sys.stderr)
                                print(f"'{cleaned_content}'", file=sys.stderr)
                                print(f"-------------------------------------------------", file=sys.stderr)

                            if cleaned_content:
                                # Print content directly. Python's print() handles newlines correctly.
                                #print(cleaned_content, end="", flush=True)
                                output_acc += cleaned_content
                                markdown_output.update(output_acc)

                        except json.JSONDecodeError as e:
                            if DEBUG_MODE:
                                print(f"--- DEBUG: JSON Decode Error: {e} for chunk: {json_chunk_str}", file=sys.stderr)
                            # This might happen for non-JSON lines or malformed chunks
                            pass # Skip malformed lines

    except requests.exceptions.ConnectionError as e:
        print(f"\nError: Could not connect to the Llamafile server at {LLAMA_SERVER_URL}.", file=sys.stderr)
        print(f"Please ensure the server is running and accessible. Details: {e}", file=sys.stderr)
    except requests.exceptions.RequestException as e:
        print(f"\nError: An unexpected request error occurred: {e}", file=sys.stderr)
    finally:
        print("", flush=True) # Add a final newline after the streamed response is complete.

# Call the function to ensure the service is running
check_and_start_service() # Corrected: added ()

man_commands_str = None
user_query_parts = []
args = sys.argv[1:]
i = 0
while i < len(args):
    arg = args[i]
    if arg == "-man":
        if i + 1 < len(args):
            man_commands_str = args[i+1]
            i += 1  # Consume the value of -man
        else:
            print("Error: -man option requires a comma-separated list of commands argument.", file=sys.stderr)
            print("Usage: ai -man <command1,command2> your query here", file=sys.stderr)
            sys.exit(1)
    else:
        user_query_parts.append(arg)
    i += 1

user_input_query = " ".join(user_query_parts)

if user_input_query:  # If there's an actual query from CLI args
    man_pages_document = ""
    if man_commands_str:
        man_pages_document = get_manpage_content(man_commands_str)

    final_query_to_llm = user_input_query
    if man_pages_document:  # Only add prefix if there's actual man content
        final_query_to_llm = f"Context from man pages:\n{man_pages_document.strip()}\n\nUser query: {user_input_query}"
    
    send_query_to_llm(final_query_to_llm)
    print("")  # For consistent spacing after output
    sys.exit(0)
elif man_commands_str:  # -man was given but no actual query string
    print("Error: -man option was provided, but no query was specified.", file=sys.stderr)
    print("Usage: ai -man <command1,command2> your query here", file=sys.stderr)
    sys.exit(1)
else:  # No CLI arguments for query, or only empty -man, proceed to interactive mode
    print("")
    print("Type your commands/questions. For short, specific answers (like shell commands), be direct.")
    print("Type 'exit' or 'quit' to end the session.")
    print("")

    # Chat loop (only if no arguments were provided that constitute a query)
    while True:
        try:
            user_input_interactive = input("> ")
        except EOFError:  # Handle Ctrl+D
            print("\nExiting chat. Goodbye!")
            break

        if user_input_interactive.lower() in ["exit", "quit"]:
            print("Exiting chat. Goodbye!")
            break

        send_query_to_llm(user_input_interactive)  # Interactive mode doesn't use -man
        print("")


