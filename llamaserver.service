[Unit]
Description=Llamafile Server
After=network.target

[Service]
# Working directory for the llamafile executable.
# Change this to the directory where your llamafile executable and model are located.
WorkingDirectory=%h/llamafiles/

# The command to execute.
# - Replace /opt/llamafile/llamafile with the actual path to your llamafile executable.
# - Replace /opt/llamafile/model.gguf with the path to your Llama model file.
# - The --port 8080 argument sets the server port. Change if needed.
# - The --host 0.0.0.0 argument makes the server accessible from all network interfaces.
#   For production, you might want to bind to a specific IP address or '127.0.0.1' if only local access is needed.
ExecStart=/bin/bash -c './google_gemma-3-1b-it-Q6_K.llamafile --port 4141 --host 127.0.0.1 --server'


# Standard output and error redirection.
# You can change these paths to your preferred log locations.
StandardOutput=journal
StandardError=journal

[Install]
# This specifies when the service should be started.
# multi-user.target means it starts when the system reaches a multi-user environment (normal boot).
WantedBy=default.target
